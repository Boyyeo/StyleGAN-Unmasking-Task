{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"validation_baseline.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1fUAjOFUgmD","executionInfo":{"status":"ok","timestamp":1641764503648,"user_tz":-480,"elapsed":13164,"user":{"displayName":"shareacc hs","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gis-3n71BEVUS-hBXn9Yk54p8FIgXVNTm2sFZL0=s64","userId":"15102280859354683372"}},"outputId":"ed2826e2-7c91-42dc-caaa-86c905084b41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3lfm2FIUFdH"},"outputs":[],"source":["!pip install deepface"]},{"cell_type":"code","source":["\n","#replace file to mute warning\n","from deepface import DeepFace\n","import gdown\n","import os\n","import shutil\n","url = \"https://drive.google.com/u/0/uc?id=1aS5SJCK9MLMyYPXmhAkwtn3cJhQGCbGj&export=download\"\n","gdown.download(url, \"/content/sample_data/DeepFace.py\")\n","os.remove(\"/usr/local/lib/python3.7/dist-packages/deepface/DeepFace.py\")\n","shutil.move(\"/content/sample_data/DeepFace.py\",\"/usr/local/lib/python3.7/dist-packages/deepface/\")\n"],"metadata":{"id":"uy3EVsZUUbzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from deepface import DeepFace\n","import gdown\n","import os\n","import shutil"],"metadata":{"id":"MqaDjznWypeS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!unzip /content/drive/Shareddrives/AIProject/Dataset/AAFD_train_unmasked_folder.zip -d /content/sample_data\n","!unzip /content/drive/Shareddrives/AIProject/Dataset/AAFD_train_masked.zip -d /content/sample_data"],"metadata":{"id":"XE9mrRASUS-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from os import listdir\n","from os.path import join, isfile"],"metadata":{"id":"puC-KvhN0jB1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def alignfolder(unmasked_path,masked_path):\n","  files = listdir(unmasked_path)\n","\n","  count = 0;\n","  for file in files:\n","    if not os.path.isfile(masked_path+\"/\"+file.split(\"_\")[0]+\".jpg\"):\n","      print(file,\" not exist\")\n","      os.remove(unmasked_path+\"/\"+file)\n","      count+=1\n","\n","  print('total deleted :',count)    \n","\n","\n","alignfolder('/content/sample_data/AAFD_train_masked','/content/sample_data/AAFD_train_masked_folder')\n","#alignfolder('/content/sample_data/asia_celebA_test_unmasked','/content/sample_data/asia_celebA_test_masked')\n","\n","test_unmasked = os.listdir('/content/sample_data/AAFD_train_masked')\n","test_masked = os.listdir('/content/sample_data/AAFD_train_unmasked_folder')\n","#train_unmasked = os.listdir('/content/sample_data/asia_celebA_train_unmasked')\n","#train_masked = os.listdir('/content/sample_data/asia_celebA_train_masked')\n","\n","\n","\n","print(\"length test_unmasked :\",len(test_unmasked))\n","print(\"length test_masked :\",len(test_masked))\n","#print(\"length train_unmasked :\",len(train_unmasked))\n","#print(\"length train_masked :\",len(train_masked))"],"metadata":{"id":"iweI34r0zI5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#shutil.rmtree(\"/content/sample_data/result_face\")\n","!mkdir /content/sample_data/result_face"],"metadata":{"id":"T6DsE0b6Uxm5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from argparse import Namespace\n","import time\n","import os\n","import sys\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","\n","EXPERIMENT_DATA_ARGS = {\n","    \"unmasked_masked\": {\n","        \"model_path\": \"pretrained_models/unmasked_masked.pt\",\n","        \"image_path\": \"notebooks/images/input_img.jpg\",\n","        \"transform\": transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","    }\n","}\n","\n","EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[\"unmasked_masked\"]\n","img_transforms = EXPERIMENT_ARGS['transform']\n","resize_dims = (256, 256)"],"metadata":{"id":"M3Dau_iVVsRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_distance(image_name,df):\n","  distance = -1\n","  for i in range(len(df['VGG-Face_cosine'])):\n","    if(image_name == df['identity'][i].split(\"/\")[4] ):\n","      distance = df['VGG-Face_cosine'][i]    \n","\n","  if(distance == -1):\n","    distance = 1\n","\n","  return distance  "],"metadata":{"id":"V_FeIdPiUyyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def calc_distance(image_name,df):\n","  distance = -1\n","  for i in range(len(df['VGG-Face_cosine'])):\n","    if(image_name == df['identity'][i].split(\"/\")[4] ):\n","      distance = df['VGG-Face_cosine'][i]    \n","\n","  if(distance == -1):\n","    distance = 1\n","\n","  return distance \n","N95 = 0\n","surgical = 0\n","cloth = 0\n","KN95 = 0\n","\n","from torchvision.utils import save_image\n","from PIL import *\n","resize_dims = (256, 256)\n","masked_dir = \"/content/sample_data/AAFD_train_masked/\"\n","db_path = \"/content/sample_data/AAFD_train_unmasked_folder\"\n","count = 0\n","total_accuracy = 0\n","for masked_image_dir in os.listdir(masked_dir):\n","  if not masked_image_dir.endswith(\".ipynb_checkpoints\"):\n","    count += 1\n","    result_dir = masked_dir + masked_image_dir\n","    with torch.no_grad():\n","      tic = time.time()\n","      result_dir = masked_dir + masked_image_dir\n","      toc = time.time()\n","      df = DeepFace.find(img_path=result_dir,db_path=db_path,enforce_detection=False)\n","      accuracy = 1 - calc_distance(masked_image_dir.split(\".\")[0].split(\"_\")[0],df)\n","      total_accuracy += accuracy\n","      if(accuracy == 0):\n","        mask_type = masked_image_dir.split(\"_\")[1]\n","        if mask_type == \"N95.jpg\":\n","          N95 +=1\n","        elif mask_type == \"KN95.jpg\":\n","          KN95 +=1\n","        elif mask_type == \"surgical.jpg\":\n","          surgical +=1\n","        elif mask_type == \"cloth.jpg\":\n","          cloth +=1      \n","\n","      print(\"masked_image :\",masked_image_dir,\"recognize_image:\",df['identity'][0].split(\"/\")[4],\"Accuracy: \",accuracy)\n","      print(\"N95 :{} KN95 :{}  surgical :{} cloth:{} over image :{}\".format(N95,KN95,surgical,cloth,count))\n","\n","print(\"Accuracy of Model over {} images is : {}\".format(count,total_accuracy/count))"],"metadata":{"id":"CxDcpbqrU3CV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ground_truth ffhq 0.6971544336001714"],"metadata":{"id":"ROVQTqx6OVDz"},"execution_count":null,"outputs":[]}]}